{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba0fefa8",
   "metadata": {},
   "source": [
    "## Model Sucks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6131ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29ca763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/williamsilver/Ai in materials workshop/AI-in-materials-workshop/AI-in-materials-workshop/mmc2_cleaned_2_no_zero_pce.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0075429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- helpers ----------\n",
    "def one_hot(value, choices):\n",
    "    \"\"\"One-hot with an extra 'other' bucket.\"\"\"\n",
    "    out = [0.0] * (len(choices) + 1)\n",
    "    idx = choices.index(value) if value in choices else len(choices)\n",
    "    out[idx] = 1.0\n",
    "    return out\n",
    "\n",
    "def index_add(src, index, dim_size):\n",
    "    \"\"\"\n",
    "    src: (E, hidden)\n",
    "    index: (E,)  destination indices\n",
    "    returns: (dim_size, hidden)\n",
    "    \"\"\"\n",
    "    out = torch.zeros(\n",
    "        dim_size,\n",
    "        src.size(1),\n",
    "        device=src.device,\n",
    "        dtype=src.dtype\n",
    "    )\n",
    "    out.index_add_(0, index, src)\n",
    "    return out\n",
    "def safe_float(x, default=0.0):\n",
    "    try:\n",
    "        if x is None: \n",
    "            return default\n",
    "        if isinstance(x, str):\n",
    "            x = float(x)\n",
    "        if np.isnan(x) or np.isinf(x):\n",
    "            return default\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f138f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- atom features ----------\n",
    "HYBRID_CHOICES = [\n",
    "    Chem.rdchem.HybridizationType.S,\n",
    "    Chem.rdchem.HybridizationType.SP,\n",
    "    Chem.rdchem.HybridizationType.SP2,\n",
    "    Chem.rdchem.HybridizationType.SP3,\n",
    "    Chem.rdchem.HybridizationType.SP3D,\n",
    "    Chem.rdchem.HybridizationType.SP3D2,\n",
    "]\n",
    "CHIRAL_CHOICES = [\n",
    "    Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "    Chem.rdchem.ChiralType.CHI_OTHER,\n",
    "]\n",
    "\n",
    "def get_atom_features(atom, gasteiger_charge=0.0):\n",
    "    \"\"\"\n",
    "    Chemprop-ish: categorical one-hots + a couple numeric signals.\n",
    "    Returns a float list.\n",
    "    \"\"\"\n",
    "    Z = atom.GetAtomicNum()\n",
    "    Z = int(Z) if Z is not None else 0\n",
    "    # atomic number one-hot (1..100), with 0 as \"unknown\"\n",
    "    Z_oh = [0.0] * 101\n",
    "    Z_oh[min(max(Z, 0), 100)] = 1.0\n",
    "\n",
    "    degree_oh = one_hot(atom.GetTotalDegree(), [0,1,2,3,4,5])\n",
    "    formal_charge_oh = one_hot(atom.GetFormalCharge(), [-3,-2,-1,0,1,2,3])\n",
    "    hybrid_oh = one_hot(atom.GetHybridization(), HYBRID_CHOICES)\n",
    "    chiral_oh = one_hot(atom.GetChiralTag(), CHIRAL_CHOICES)\n",
    "\n",
    "    total_h_oh = one_hot(atom.GetTotalNumHs(), [0,1,2,3,4])\n",
    "    implicit_val_oh = one_hot(atom.GetImplicitValence(), [0,1,2,3,4,5,6])\n",
    "    explicit_val_oh = one_hot(atom.GetExplicitValence(), [0,1,2,3,4,5,6])\n",
    "    radical_oh = one_hot(atom.GetNumRadicalElectrons(), [0,1,2])\n",
    "\n",
    "    is_aromatic = [1.0 if atom.GetIsAromatic() else 0.0]\n",
    "    in_ring = [1.0 if atom.IsInRing() else 0.0]\n",
    "\n",
    "    # Numeric features\n",
    "    mass = [atom.GetMass() / 200.0]  # roughly scaled\n",
    "    # clamp charge to a reasonable range\n",
    "    q = float(np.clip(safe_float(gasteiger_charge, 0.0), -2.0, 2.0))\n",
    "    g_charge = [q]\n",
    "\n",
    "    feats = (\n",
    "        Z_oh + degree_oh + formal_charge_oh + hybrid_oh + chiral_oh +\n",
    "        total_h_oh + implicit_val_oh + explicit_val_oh + radical_oh +\n",
    "        is_aromatic + in_ring +\n",
    "        mass + g_charge\n",
    "    )\n",
    "    return feats\n",
    "\n",
    "# ---------- bond features ----------\n",
    "STEREO_CHOICES = [\n",
    "    Chem.rdchem.BondStereo.STEREONONE,\n",
    "    Chem.rdchem.BondStereo.STEREOZ,\n",
    "    Chem.rdchem.BondStereo.STEREOE,\n",
    "    Chem.rdchem.BondStereo.STEREOCIS,\n",
    "    Chem.rdchem.BondStereo.STEREOTRANS,\n",
    "    Chem.rdchem.BondStereo.STEREOANY,\n",
    "]\n",
    "BONDDIR_CHOICES = [\n",
    "    Chem.rdchem.BondDir.NONE,\n",
    "    Chem.rdchem.BondDir.ENDUPRIGHT,\n",
    "    Chem.rdchem.BondDir.ENDDOWNRIGHT,\n",
    "]\n",
    "\n",
    "def get_bond_features(bond):\n",
    "    bt = bond.GetBondType()\n",
    "    bond_type_oh = [\n",
    "        1.0 if bt == Chem.rdchem.BondType.SINGLE else 0.0,\n",
    "        1.0 if bt == Chem.rdchem.BondType.DOUBLE else 0.0,\n",
    "        1.0 if bt == Chem.rdchem.BondType.TRIPLE else 0.0,\n",
    "        1.0 if bt == Chem.rdchem.BondType.AROMATIC else 0.0,\n",
    "    ]\n",
    "    conj = [1.0 if bond.GetIsConjugated() else 0.0]\n",
    "    ring = [1.0 if bond.IsInRing() else 0.0]\n",
    "    stereo_oh = one_hot(bond.GetStereo(), STEREO_CHOICES)\n",
    "    dir_oh = one_hot(bond.GetBondDir(), BONDDIR_CHOICES)\n",
    "\n",
    "    return bond_type_oh + conj + ring + stereo_oh + dir_oh\n",
    "\n",
    "def smiles_to_graph(smiles, targets):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "\n",
    "    # Compute Gasteiger charges (best on H-added molecule)\n",
    "    molH = Chem.AddHs(mol)\n",
    "    try:\n",
    "        AllChem.ComputeGasteigerCharges(molH)\n",
    "        charges = []\n",
    "        for i in range(mol.GetNumAtoms()):\n",
    "            a = molH.GetAtomWithIdx(i)\n",
    "            charges.append(a.GetProp(\"_GasteigerCharge\"))\n",
    "    except Exception:\n",
    "        charges = [0.0] * mol.GetNumAtoms()\n",
    "\n",
    "    # Node features\n",
    "    node_feats = []\n",
    "    for i, atom in enumerate(mol.GetAtoms()):\n",
    "        q = charges[i] if i < len(charges) else 0.0\n",
    "        node_feats.append(get_atom_features(atom, gasteiger_charge=q))\n",
    "    x = torch.tensor(node_feats, dtype=torch.float)\n",
    "\n",
    "    # Directed edges + edge features + reverse edge mapping\n",
    "    edge_pairs = []\n",
    "    edge_feats = []\n",
    "    pair_to_idx = {}\n",
    "\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        bf = get_bond_features(bond)\n",
    "\n",
    "        # i -> j\n",
    "        pair_to_idx[(i, j)] = len(edge_pairs)\n",
    "        edge_pairs.append([i, j])\n",
    "        edge_feats.append(bf)\n",
    "\n",
    "        # j -> i\n",
    "        pair_to_idx[(j, i)] = len(edge_pairs)\n",
    "        edge_pairs.append([j, i])\n",
    "        edge_feats.append(bf)\n",
    "\n",
    "    if len(edge_pairs) == 0:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        edge_attr = torch.empty((0, 0), dtype=torch.float)  # will be fixed below\n",
    "        rev_edge_index = torch.empty((0,), dtype=torch.long)\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_pairs, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_feats, dtype=torch.float)\n",
    "        rev_edge_index = torch.tensor(\n",
    "            [pair_to_idx[(dst, src)] for src, dst in edge_pairs],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "\n",
    "    y = torch.tensor([targets], dtype=torch.float)  # (1,4)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "    data.rev_edge_index = rev_edge_index  # store for D-MPNN\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03fb678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48357/48357 [01:19<00:00, 606.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 48357 graph objects.\n",
      "Node feature dim: 158\n",
      "Edge feature dim: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "final_graphs = []\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    targets = [row[\"homo\"], row[\"lumo\"], row[\"gap\"], row[\"pce\"]]\n",
    "    g = smiles_to_graph(row[\"smiles\"], targets)\n",
    "    if g is not None and g.x is not None:\n",
    "        # If a molecule has no bonds, edge_attr will be empty; we handle in model\n",
    "        final_graphs.append(g)\n",
    "\n",
    "graph_list = final_graphs\n",
    "print(f\"Created {len(graph_list)} graph objects.\")\n",
    "print(\"Node feature dim:\", graph_list[0].x.shape[1])\n",
    "print(\"Edge feature dim:\", (graph_list[0].edge_attr.shape[1] if graph_list[0].edge_attr.numel() else \"empty\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c86fdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import Set2Set\n",
    "\n",
    "class DMPNN_Set2Set(nn.Module):\n",
    "    def __init__(self, node_in_dim, edge_in_dim, hidden_dim=256, depth=3, out_dim=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.depth = depth\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.W_in = nn.Linear(node_in_dim + edge_in_dim, hidden_dim)\n",
    "        self.W_msg = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W_atom = nn.Linear(node_in_dim + hidden_dim, hidden_dim)\n",
    "\n",
    "        self.pool = Set2Set(hidden_dim, processing_steps=3)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "        batch = data.batch\n",
    "        N = x.size(0)\n",
    "\n",
    "        # Handle molecules with no bonds\n",
    "        if edge_index.numel() == 0:\n",
    "            atom_h = F.relu(\n",
    "                self.W_atom(\n",
    "                    torch.cat([x, torch.zeros((N, self.hidden_dim), device=x.device)], dim=1)\n",
    "                )\n",
    "            )\n",
    "            pooled = self.pool(atom_h, batch)\n",
    "            return self.head(pooled)\n",
    "\n",
    "        src, dst = edge_index\n",
    "        rev = data.rev_edge_index.to(x.device)\n",
    "\n",
    "        # Initial edge embeddings\n",
    "        h0 = F.relu(self.W_in(torch.cat([x[src], edge_attr], dim=1)))\n",
    "        h = h0\n",
    "\n",
    "        for _ in range(self.depth):\n",
    "            # m_in = scatter_add(h, dst)\n",
    "            m_in = index_add(h, dst, N)\n",
    "\n",
    "            # remove reverse edge\n",
    "            m_e = m_in[src] - h[rev]\n",
    "\n",
    "            h = F.relu(self.W_msg(h0 + m_e))\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Atom-level aggregation\n",
    "        atom_msg = index_add(h, dst, N)\n",
    "        atom_h = F.relu(self.W_atom(torch.cat([x, atom_msg], dim=1)))\n",
    "\n",
    "        pooled = self.pool(atom_h, batch)\n",
    "        return self.head(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44bea101",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        pred = model(batch)\n",
    "        y = batch.y.view(pred.shape).to(device)\n",
    "        loss = criterion(pred, y)\n",
    "        total += loss.item() * batch.num_graphs\n",
    "        n += batch.num_graphs\n",
    "    return total / max(n, 1)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch)\n",
    "        y = batch.y.view(pred.shape).to(device)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        total += loss.item() * batch.num_graphs\n",
    "        n += batch.num_graphs\n",
    "    return total / max(n, 1)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, min_delta=1e-4):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best = float(\"inf\")\n",
    "        self.count = 0\n",
    "\n",
    "    def step(self, val_loss):\n",
    "        improved = (self.best - val_loss) > self.min_delta\n",
    "        if improved:\n",
    "            self.best = val_loss\n",
    "            self.count = 0\n",
    "            return False  # don't stop\n",
    "        else:\n",
    "            self.count += 1\n",
    "            return self.count >= self.patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01cee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | LR 1.00e-03 | Train MSE 0.3543 | Val MSE 0.2384\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Safety: only Data objects\n",
    "graph_list = [g for g in graph_list if isinstance(g, Data)]\n",
    "assert len(graph_list) > 0, \"No valid graphs found.\"\n",
    "\n",
    "node_in_dim = graph_list[0].x.shape[1]\n",
    "# edge dim may be empty for some molecules; infer from a molecule that has edges\n",
    "edge_in_dim = None\n",
    "for g in graph_list:\n",
    "    if g.edge_attr is not None and g.edge_attr.numel() > 0:\n",
    "        edge_in_dim = g.edge_attr.shape[1]\n",
    "        break\n",
    "assert edge_in_dim is not None, \"All molecules have no bonds? Edge features couldn't be inferred.\"\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fold_best = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(graph_list), start=1):\n",
    "    print(f\"\\n--- Fold {fold}/5 ---\")\n",
    "\n",
    "    train_set = [copy.deepcopy(graph_list[i]) for i in train_idx]\n",
    "    val_set   = [copy.deepcopy(graph_list[i]) for i in val_idx]\n",
    "\n",
    "    # Fit scaler on TRAIN targets only\n",
    "    train_y_raw = np.vstack([g.y.detach().cpu().numpy() for g in train_set])  # (N,4)\n",
    "    scaler = StandardScaler().fit(train_y_raw)\n",
    "\n",
    "    # Apply scaling\n",
    "    for g in train_set:\n",
    "        g.y = torch.tensor(scaler.transform(g.y.detach().cpu().numpy()), dtype=torch.float)\n",
    "\n",
    "    for g in val_set:\n",
    "        g.y = torch.tensor(scaler.transform(g.y.detach().cpu().numpy()), dtype=torch.float)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    val_loader   = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "\n",
    "    model = DMPNN_Set2Set(\n",
    "        node_in_dim=node_in_dim,\n",
    "        edge_in_dim=edge_in_dim,\n",
    "        hidden_dim=256,\n",
    "        depth=3,\n",
    "        out_dim=4,\n",
    "        dropout=0.1,\n",
    "    ).to(device)\n",
    "\n",
    "    # Weight decay: try 1e-5 then 1e-4 if needed\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"min\",\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-5,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    early = EarlyStopping(patience=15, min_delta=1e-4)\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, 101):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss   = evaluate(model, val_loader, criterion, device)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            print(f\"Epoch {epoch:03d} | LR {lr:.2e} | Train MSE {train_loss:.4f} | Val MSE {val_loss:.4f}\")\n",
    "\n",
    "        if early.step(val_loss):\n",
    "            print(f\"Early stopping at epoch {epoch} (best val {best_val:.4f})\")\n",
    "            break\n",
    "\n",
    "    # restore best weights for this fold\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "\n",
    "    fold_best.append(best_val)\n",
    "\n",
    "print(f\"\\nAverage 5-Fold best Val MSE: {np.mean(fold_best):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc51d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
